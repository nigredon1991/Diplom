1) Меня зовут ..., мой научный руководитель .... Тема моей работы «Анализ алгоритмов обучения нейронных сетей на примере решения задачи распознавания рукописных цифр».
2)
Градиентный спуск - это самый популярный алгоритм оптимизации, также один из главных методов для обучения искусственных нейронных сетей. В настоящее время существует несколько библиотек машинного обучения, в которых реализованы различные методы оптимизации. Например keras\cite{bib:keras}, caffe\cite{bib:caffe}. Однако эти алгоритмы используются в качестве "чёрного ящика", и нет возможности понять сильные и слабые стороны каждого алгоритма оптимизации. При этом зачастую от правильного выбора метода зависит качество решения конкретной прикладной задачи. Иначе, возможно увеличение времени расчетов и непредсказуемого поведения. Таким образом, задача выбора требуемого алгоритма оптимизации градиентного спуска является актуальной и её решение представляет значительный интерес.

3)Задача распознавания рукописных цифр является актуальной на сегодняшний день. Исследователи со всего мира предлагают различные методы для распознавания рукописных цифр, написанные на различных языках.  На слайде изображен типичный пример арабской цифры, которую необходимо распознать.

4) Одним из интенсивно используемых подходов для решения этой задачи является применение нейронных сетей. Поскольку они обладают такими преимуществами как
    \begin{itemize}
        \item Адаптивность.
        \item Масштабируемость.
        \item Параллельная обработка информации.
        \item Запоминание контекстной информацию.
    \end{itemize}

5) Таким образом, целью моей работы является сравнительное исследование и анализ эффективности различных методов оптимизации при обучении нейронных сетей для решения задачи распознавания рукописных арабских цифр. Тогда правильный выбор алгоритма обучения позволил бы решать эту задачу не прибегая к разработке дополнительных более сложных алгоритмов распознавания цифр.

6) Мной в рамках данной работы были рассмотрена однослойный персептрон Розенблатта и многослойные нейронные сети.

7) А также следующие известные методы оптимизации, которые были использованы для обучения нейронных сетей
    \begin{itemize}
    \item Метод стохастического градиента с инерцией(Nesterov Accelerated Gradient).
    \item Метод адаптивного градиента (Adagrad).
    \item Метод адаптивного скользящего среднего градиентов (RMSProp).
    \item Метод адаптивного шага обучения(Adadelta).
    \item Метод адаптивной инерции (Adam).
    \end{itemize}
8)
Для примера рассмотрим
Метод адаптивного градиента (Adagrad).
Некоторые признаки могут быть крайне информативными, но встречаться редко. Особый графический узор, либо необычное слово. Требуется проверять насколько часто обновляется каждый параметр в отдельности, учитываю историю всех предыдущих градиентов. Идея масштабирования. Это делается с помощью деления каждого элемента в градиенте на квадратный корень суммы квадратов прошлых соответстующих элементов градиента. Формула пересчёта:
\[g_{t+1} = g_t + \nabla f(w_t)^2 \]
\[w_{t+1} = w_t - \frac{\mu \nabla f(w_t) }{\sqrt{g_{t+1} + \epsilon)} }  \]
Здесь \(\epsilon\) требуется, чтобы не было ошибки деления на ноль. У часто обновляющегося в прошлом параметра значение \(g_t \) будет в знаменателе очень большим, там самым будет меняться меньше, чем у параметра, который поменялся меньше раз. Идея алгоритма в том, чтобы уменьшать обновление элементов, которые и так часто обновляются, и находить редкие параметры. Ещё одним достоинством Adagrad  является отсутствие необходмости точно подбирать скорость обучения.

9) Для достижения цели работы мной была написана программа и произведен вычислительный эксперимент. В программе были реализованы указанные нейронные сети и методы оптимизации, а также модуль сбора статистики. Программа написана на языке питон. Для тестирования методов были использованы тестовые данные с открытой площадки для соревнований по машинному обучения. Данный набор данных содержит 42 000 размеченных чёрно-белых изображений размером 26*26 пикселей, которые поступали на вход нейронной сети.

Обучение нейронных сетей происходит на 3/4 выборки (30000 примеров выбранных случайно), проверка сети осуществлена на 1000 тестовых примеров взятых случайно. Также мы замеряем время обучения - в среднем оно происходит в пределах одной минуты.

10) Результаты

На слайде приведены результаты программы для каждого метода оптимизации. В нём сначала написано название метода, потом время вычислений в секундах и относительная величина правильных ответов на тестовой выборке, состоящей из тысячи примеров.

Приведёны примеры процесса обучения для следующих алгоритмов:

11)Процесс обучения для метода стохастического градиента с инерцией (Нестеров)
Как видно на графике, скорость обучения довольно большая, примерно после 2000 примеров процент правильных ответов доходит до 90\%. 

12) Процесс обучения для метода адаптивного шага обучения(Adadelta)
На этом методе скорость обучения медленнее, доходим до 90\% после примерно 24000 примеров. 

13) Процесс обучения для метода адаптивного градиента(Adagrad)
Здесь показатели получились наилучшими, уже после первых 200 примеров достигли 70\% точности. 

14) Как видно из рисунков, процент правильных ответов колеблется вокруг 90\%. Это происходит, потому что на каждой проверке тестовая выборка меняется, а на каких-то примерах результат может быть хуже чем на других. В основном уже после 3000 примеров\ttfamily accurance \normalfont становится более 95\%. При проведённых тестах наилучший результат по точности показал метод \ttfamily Adagrad\normalfont. После проведения первых тестов значение \ttfamily accurance \normalfont достигло 0,7, а конечный результат составил 94\%, что на 3 процента больше, чем ближайший к нему метод. Метод стохастического градиента является наилучшим по времени и по соотношению точность/время. По точности этот метод уступает методу адаптивного градиента.

Таким образом, основная цель работы была достигнута. Было проведено сравнительное исследование различных методов оптимизации и выявлены метод, показывающие наилучшие результаты для задачи распознавания рукописных цифр.

15) Мне осталось только указать источники, используемые в работе.